# FFHQ-256 DhariwalUNet (EDM) Configuration
model_type: adm
image_size: 256
f: 8
num_in_channels: 4
num_out_channels: 4
nf: 256
num_res_blocks: 2
attn_resolutions:
  - 16
  - 8
  - 4
dropout: 0.0
ch_mult:
  - 1
  - 2
  - 3
  - 4
resamp_with_conv: true
num_heads: 4
num_head_channels: -1
num_heads_upsample: -1
use_scale_shift_norm: true
resblock_updown: false
use_new_attention_order: false
num_classes: 1
label_dim: 0
label_dropout: 0.0
use_origin_adm: false


# # ADM UNet configuration 

# image_size: 256              # Full image resolution; internally, UNet gets 256//8 = 32
# f: 8                        # Downsample factor from the autoencoder (256/32 = 8)   
# num_in_channels: 4           # Input channels (e.g., latent space channels from the autoencoder)
# num_out_channels: 4          # Output channels
# nf: 256                    # Base channel count (model_channels)
# num_res_blocks: 2          # Number of residual blocks per scale
# attn_resolutions:          # Spatial resolutions (downsample factors) at which attention is applied
#   - 16
#   - 8
#   - 4
# dropout: 0.0               # No dropout
# ch_mult:                   # Channel multipliers for each level of the UNet
#   - 1
#   - 2
#   - 3                   
#   - 4                     # 4 # SHAHRIAR ADD
# resamp_with_conv: true     # Use convolutional resampling (upsample/downsample)
# num_heads: 4               # Number of attention heads in each attention block
# num_head_channels: -1      # Use default behavior (compute head width from num_heads and channel count)
# num_heads_upsample: -1     # Same as num_heads for upsampling layers
# use_scale_shift_norm: true # Use FiLM-style conditioning in the normalization layers
# resblock_updown: false     # Do not use specialized up/down residual blocks (use simple Downsample/Upsample)
# use_new_attention_order: false  # Use the legacy attention order
# num_classes: null          # Unconditional model (set to an integer for class-conditional models)
# label_dim: 0  # Unconditional model
# label_dropout: 0.0  # No label dropout
